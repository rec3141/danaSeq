// DuckDB integration: load pipeline outputs into dana.duckdb
// Runs R scripts that expect a specific directory layout with publishDir outputs.
// Uses val(barcode_dir) since R scripts operate on the published output directory
// (host filesystem), not the Nextflow work directory.
// maxForks=1 to prevent concurrent DuckDB access

// Canonical 136 reverse-complement-collapsed tetranucleotide column names
// Generated by tetramer_freqs_esom.pl; fixed set used for tnfs.txt header
def TETRA_COLS = 'seqid\tAAAA\tAAAC\tAAAG\tAAAT\tAACA\tAACC\tAACG\tAACT\tAAGA\tAAGC\tAAGG\tAAGT\tAATA\tAATC\tAATG\tAATT\tACAA\tACAC\tACAG\tACAT\tACCA\tACCC\tACCG\tACCT\tACGA\tACGC\tACGG\tACGT\tACTA\tACTC\tACTG\tAGAA\tAGAC\tAGAG\tAGAT\tAGCA\tAGCC\tAGCG\tAGCT\tAGGA\tAGGC\tAGGG\tAGTA\tAGTC\tAGTG\tATAA\tATAC\tATAG\tATAT\tATCA\tATCC\tATCG\tATGA\tATGC\tATGG\tATTA\tATTC\tATTG\tCAAA\tCAAC\tCAAG\tCACA\tCACC\tCACG\tCAGA\tCAGC\tCAGG\tCATA\tCATC\tCATG\tCCAA\tCCAC\tCCAG\tCCCA\tCCCC\tCCCG\tCCGA\tCCGC\tCCGG\tCCTA\tCCTC\tCGAA\tCGAC\tCGAG\tCGCA\tCGCC\tCGCG\tCGGA\tCGGC\tCGTA\tCGTC\tCTAA\tCTAC\tCTAG\tCTCA\tCTCC\tCTGA\tCTGC\tCTTA\tCTTC\tGAAA\tGAAC\tGACA\tGACC\tGAGA\tGAGC\tGATA\tGATC\tGCAA\tGCAC\tGCCA\tGCCC\tGCGA\tGCGC\tGCTA\tGGAA\tGGAC\tGGCA\tGGCC\tGGGA\tGGTA\tGTAA\tGTAC\tGTCA\tGTGA\tGTTA\tTAAA\tTACA\tTAGA\tTATA\tTCAA\tTCCA\tTCGA\tTGAA\tTGCA\tTTAA'

process DB_INTEGRATION {
    tag "${barcode_dir}"
    label 'process_medium'
    conda "${projectDir}/conda-envs/dana-tools"
    maxForks 1
    executor 'local'

    input:
    val barcode_dir

    output:
    val barcode_dir

    script:
    """
    # R scripts do setwd(args[1]) then open dana.duckdb in that directory
    # They expect kraken/, prokka/, sketch/, tetra/ subdirectories
    # Patch hardcoded source("/work/apps/dana/log-db.r") to use danadir

    run_r_script() {
        local script="\$1"; shift
        local patched=\$(mktemp)
        sed -e 's|source("/work/apps/dana/log-db.r")|source("'${params.danadir}'/46_log_db.r")|' \
            -e 's|library(tidyverse)|library(stringr); library(dplyr); library(tidyr); library(tibble)|' \
            "\$script" > "\$patched"
        Rscript "\$patched" "\$@" || true
        rm -f "\$patched"
    }

    if [ -d "${barcode_dir}/kraken" ] && ls "${barcode_dir}"/kraken/*.tsv >/dev/null 2>&1; then
        run_r_script ${params.danadir}/40_kraken_db.r "${barcode_dir}"
        run_r_script ${params.danadir}/41_krakenreport_db.r "${barcode_dir}"
    fi

    if [ -d "${barcode_dir}/prokka" ]; then
        run_r_script ${params.danadir}/42_prokka_db.r "${barcode_dir}"
    fi

    if [ -d "${barcode_dir}/sketch" ] && ls "${barcode_dir}"/sketch/*.txt >/dev/null 2>&1; then
        run_r_script ${params.danadir}/43_sketch_db.r "${barcode_dir}"
    fi

    if [ -d "${barcode_dir}/tetra" ] && ls "${barcode_dir}"/tetra/*.lrn >/dev/null 2>&1; then
        # Generate tnfs.txt header if missing (required by 44_tetra_db.r)
        if [ ! -s "${barcode_dir}/tnfs.txt" ]; then
            printf '${TETRA_COLS}\\n' > "${barcode_dir}/tnfs.txt"
        fi
        run_r_script ${params.danadir}/44_tetra_db.r "${barcode_dir}"
    fi
    """
}

// Post-DB cleanup: compress or delete source files after DuckDB import confirms
// results are loaded.  Runs after DB_INTEGRATION in batch mode.
// Strategy: gzip fa/ in place, delete kraken/sketch/tetra (data in DuckDB),
// delete prokka TSVs, gzip prokka .gff/.faa/.ffn.  Per-file approach is safe
// for watch mode where new files arrive between cleanup cycles.
// maxForks=1 since cleanup is I/O-bound and sequential is fine.

process CLEANUP {
    tag "${barcode_dir}"
    label 'process_low'
    conda "${projectDir}/conda-envs/dana-tools"
    maxForks 1
    executor 'local'

    input:
    val barcode_dir

    script:
    """
    DB="${barcode_dir}/dana.duckdb"
    if [ ! -f "\$DB" ]; then
        echo "[WARNING] No DuckDB at \$DB, skipping cleanup"
        exit 0
    fi

    imported_count=\$(Rscript -e "
        library(DBI); library(duckdb)
        con <- dbConnect(duckdb(), '\$DB')
        cat(dbGetQuery(con, 'SELECT count(*) FROM import_log')[[1]])
        dbDisconnect(con, shutdown=TRUE)
    " 2>/dev/null || echo 0)

    if [ "\$imported_count" -eq 0 ] 2>/dev/null; then
        echo "[WARNING] import_log empty for ${barcode_dir}, skipping cleanup"
        exit 0
    fi

    echo "[INFO] CLEANUP: \$imported_count files in import_log for ${barcode_dir}"
    before=\$(du -sh "${barcode_dir}" | cut -f1)

    # fa/: gzip FASTA files in place (not in DuckDB, keep as compressed backup)
    find "${barcode_dir}/fa" -name '*.fa' -exec gzip {} \\; 2>/dev/null || true
    echo "[INFO] Compressed fa/*.fa in place"

    # kraken/, sketch/, tetra/: delete files already loaded into DuckDB
    for subdir in kraken sketch tetra; do
        [ ! -d "${barcode_dir}/\$subdir" ] && continue
        find "${barcode_dir}/\$subdir" -type f -delete 2>/dev/null || true
        echo "[INFO] Deleted files in \$subdir/"
    done

    # prokka/: delete loaded TSVs, compress annotation files in place
    find "${barcode_dir}/prokka" -name 'PROKKA_*.tsv' -delete 2>/dev/null || true
    find "${barcode_dir}/prokka" \\( -name '*.gff' -o -name '*.faa' -o -name '*.ffn' \\) ! -name '*.gz' -exec gzip {} \\; 2>/dev/null || true
    echo "[INFO] Deleted prokka TSVs, compressed .gff/.faa/.ffn"

    after=\$(du -sh "${barcode_dir}" | cut -f1)
    echo "[INFO] Space: \$before -> \$after"
    """
}

// Periodic DB sync for watch mode.
// Runs as a long-lived process with an internal sleep loop â€” in watch mode the
// pipeline never completes (watchPath keeps the DAG alive), so this process
// runs indefinitely alongside it.  Scans the output directory for barcode
// subdirectories and loads results into DuckDB.  The R scripts are idempotent
// (they use import_log to track what's already been loaded).

process DB_SYNC {
    tag "db-sync"
    label 'process_medium'
    conda "${projectDir}/conda-envs/dana-tools"
    maxForks 1
    executor 'local'

    input:
    val outdir           // absolute path to output directory
    val danadir          // path to R scripts directory
    val sync_seconds     // sleep interval between sync cycles
    val cleanup_enabled  // "true" or "false"

    script:
    """
    run_r_script() {
        local script="\$1"; shift
        local patched=\$(mktemp)
        sed -e 's|source("/work/apps/dana/log-db.r")|source("'${danadir}'/46_log_db.r")|' \
            -e 's|library(tidyverse)|library(stringr); library(dplyr); library(tidyr); library(tibble)|' \
            "\$script" > "\$patched"
        Rscript "\$patched" "\$@" || true
        rm -f "\$patched"
    }

    run_cleanup() {
        local bdir="\$1"
        local DB="\${bdir}/dana.duckdb"
        [ ! -f "\$DB" ] && return 0

        local imported_count
        imported_count=\$(Rscript -e "
            library(DBI); library(duckdb)
            con <- dbConnect(duckdb(), '\$DB')
            cat(dbGetQuery(con, 'SELECT count(*) FROM import_log')[[1]])
            dbDisconnect(con, shutdown=TRUE)
        " 2>/dev/null || echo 0)

        [ "\$imported_count" -eq 0 ] 2>/dev/null && return 0

        echo "[INFO] CLEANUP: \$imported_count files in import_log for \$bdir"
        local before=\$(du -sh "\$bdir" | cut -f1)

        # fa/: gzip in place (not in DuckDB, keep as compressed backup)
        find "\${bdir}/fa" -name '*.fa' -exec gzip {} \\; 2>/dev/null || true

        # kraken/, sketch/, tetra/: delete files (data lives in DuckDB)
        for subdir in kraken sketch tetra; do
            [ ! -d "\${bdir}/\$subdir" ] && continue
            find "\${bdir}/\$subdir" -type f -delete 2>/dev/null || true
        done

        # prokka/: delete loaded TSVs, compress annotation files
        find "\${bdir}/prokka" -name 'PROKKA_*.tsv' -delete 2>/dev/null || true
        find "\${bdir}/prokka" \\( -name '*.gff' -o -name '*.faa' -o -name '*.ffn' \\) ! -name '*.gz' -exec gzip {} \\; 2>/dev/null || true

        local after=\$(du -sh "\$bdir" | cut -f1)
        echo "[INFO] CLEANUP \$bdir: \$before -> \$after"
    }

    tick=0
    while true; do
        echo "[INFO] DB_SYNC tick=\${tick}: scanning ${outdir} for barcode directories"

        # Find all barcode directories (outdir/flowcell/barcodeNN)
        for barcode_dir in \$(find ${outdir} -mindepth 2 -maxdepth 2 -type d -name 'barcode*' 2>/dev/null | sort); do
            echo "[INFO] DB_SYNC: processing \${barcode_dir}"

            if [ -d "\${barcode_dir}/kraken" ] && ls "\${barcode_dir}"/kraken/*.tsv >/dev/null 2>&1; then
                run_r_script ${danadir}/40_kraken_db.r "\${barcode_dir}"
                run_r_script ${danadir}/41_krakenreport_db.r "\${barcode_dir}"
            fi

            if [ -d "\${barcode_dir}/prokka" ]; then
                run_r_script ${danadir}/42_prokka_db.r "\${barcode_dir}"
            fi

            if [ -d "\${barcode_dir}/sketch" ] && ls "\${barcode_dir}"/sketch/*.txt >/dev/null 2>&1; then
                run_r_script ${danadir}/43_sketch_db.r "\${barcode_dir}"
            fi

            if [ -d "\${barcode_dir}/tetra" ] && ls "\${barcode_dir}"/tetra/*.lrn >/dev/null 2>&1; then
                if [ ! -s "\${barcode_dir}/tnfs.txt" ]; then
                    printf '${TETRA_COLS}\\n' > "\${barcode_dir}/tnfs.txt"
                fi
                run_r_script ${danadir}/44_tetra_db.r "\${barcode_dir}"
            fi

            # Post-sync cleanup if enabled
            if [ "${cleanup_enabled}" = "true" ]; then
                run_cleanup "\${barcode_dir}"
            fi
        done

        echo "[INFO] DB_SYNC tick=\${tick}: complete, sleeping ${sync_seconds}s"
        tick=\$((tick + 1))
        sleep ${sync_seconds}
    done
    """
}
